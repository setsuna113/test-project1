# #################################
# Basic training parameters for enhancement.
#
# Authors:
#  * Szu-Wei Fu 2020
#  * Chien-Feng Liao 2020
#  * Peter Plantinga 2020, 2021
# #################################

# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 12243
__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]

# Set up folders for reading from and writing to
# Dataset will be downloaded to the `data_folder`
# If you plan to train a system on an HPC cluster with a big dataset,
# we strongly suggest doing the following:
# 1- Compress the dataset in a single tar or zip file.
# 2- Copy your dataset locally (i.e., the local disk of the computing node).
# 3- Uncompress the dataset in the local folder.
# 4- Set data_folder with the local path.
# Reading data from the local disk of the compute node (e.g. $SLURM_TMPDIR with SLURM-based clusters) is very important.
# It allows you to read the data much faster without slowing down the shared filesystem.
project_name: xxxxx
curr_path: /home/jason/speechbrain/templates/DTLN
data_folder: !ref <curr_path>/audio_cache
data_untar_folder: /home/jason/data
output_folder: !ref <curr_path>/results/<seed>
save_folder: !ref <output_folder>/save/data
train_log: !ref <output_folder>/train_log.txt

# Path where data manifest files will be stored
# The data manifest files are created by the data preparation script.
train_annotation: !ref <data_folder>/train.csv
valid_annotation: !ref <data_folder>/valid.csv
test_annotation: !ref <data_folder>/test.csv
metric_annotation: !ref <data_folder>/metric.csv

# FFT parameters
sample_rate: 16000
win_length: 512 # sample
hop_length: 128 # sample
window_fn: !name:torch.hamming_window

# audio len
length_per_sample: 10

noisy_speech:
    sampling_rate: !ref <sample_rate>
    audioformat: '*.wav'
    audio_length: 10
    silence_length: 0.2
    total_hours: 1
    snr_lower: -5
    snr_upper: 20
    randomize_snr: True
    target_level_lower: -35
    target_level_upper: -15
    total_snrlevels: 21
    clean_activity_threshold: 0.6
    noise_activity_threshold: 0.0
    fileindex_start: None
    fileindex_end: None
    is_test_set: False

    noise_dir: !ref <data_untar_folder>/noise
    speech_dir: !ref <data_untar_folder>/clean
    testset_dir: !ref <data_untar_folder>/dns3/datasets/test_set/synthetic

    noisy_destination: !ref <data_folder>/training_set/noisy
    clean_destination: !ref <data_folder>/training_set/clean
    noise_destination: !ref <data_folder>/training_set/noise
    rir_destination: !ref <data_folder>/training_set/rir
    train_list_file: !ref <data_folder>/train.list
    log_dir: !ref <data_folder>/logs

    # Config: add singing voice to clean speech
    use_singing_data: 0
    # 0 for no, 1 for yes
    clean_singing: !ref <data_untar_folder>/dns3/datasets/clean/singing_voice
    #datasets\clean_singing\VocalSet11\FULL
    singing_choice: 3
    # 1 for only male, 2 for only female, 3 (default) for both male and female

    # Config: add emotional data to clean speech
    use_emotion_data: 0
    # 0 for no, 1 for yes
    clean_emotion: !ref <data_untar_folder>/dns3/datasets/clean/emotional_speech

    # Config: add Chinese (mandarin) data to clean speech
    use_mandarin_data: 0
    # 0 for no, 1 for yes
    clean_mandarin: !ref <data_untar_folder>/dns3/datasets/clean/mandarin_speech

    add_reverb_data: 0
    rir_dir: !ref <data_untar_folder>/dns3/datasets/impulse_responses
    # Config: add reverb to clean speech
    rir_choice: 3
    # 1 for only real rir, 2 for only synthetic rir, 3 (default) use both real and synthetic
    lower_t60: 0.3 
    # lower bound of t60 range in seconds
    upper_t60: 1.3 
    # upper bound of t60 range in seconds
    percent_for_adding_reverb: 1.0 # percentage of clean speech convolved with RIR

    # Unit tests config
    snr_test: True
    norm_test: True
    sampling_rate_test: True
    clipping_test: True

    unit_tests_log_dir: !ref <data_folder>/unittests_logs

# The train logger writes training statistics to a file, as well as stdout.
train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>


# Training Parameters
number_of_epochs: 50
batch_size: 15
num_workers: 8
learning_rate: 0.001
dataloader_options:
    batch_size: !ref <batch_size>

# test metrics
metrics_list: [WB_PESQ, NB_PESQ, STOI, SI_SDR]

# To design a FRCRN model, either just edit the simple FRCRN
# class that's listed here, or replace this `!new` call with a line
# pointing to a different file you've defined.
model: !new:gtcrn.GTCRN
    frame_len: !ref <win_length>
    frame_hop: !ref <hop_length>
    window: hann

# The first object passed to the Brain class is this "Epoch Counter"
# which is saved by the Checkpointer so that training can be resumed
# if it gets interrupted at any point.
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

# Objects in "modules" dict will have their parameters moved to the correct
# device, as well as having train()/eval() called on them by the Brain class.
modules:
    model: !ref <model>

# This optimizer will be constructed by the Brain class after all parameters
# are moved to the correct device. Then it will be added to the checkpointer.
opt_class: !name:torch.optim.Adam
    lr: !ref <learning_rate>
    weight_decay: 0.00001

# This object is used for saving the state of training both so that it
# can be resumed if it gets interrupted, and also so that the best checkpoint
# can be later loaded for evaluation or inference.
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        model: !ref <model>
        counter: !ref <epoch_counter>
