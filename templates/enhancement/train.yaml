seed: 4234
__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]

# Project and original dataset
project_name: xxxxx
curr_path: /home/jason/speechbrain/templates/enhancement
data_untar_folder: /home/jason/data

# Prepare training data
data_folder: !ref <curr_path>/audio_cache
train_list_file: !ref <data_folder>/train.list
train_annotation: !ref <data_folder>/train.json
valid_annotation: !ref <data_folder>/valid.json
test_annotation: !ref <data_folder>/test.json
skip_prep: False

# Ckpt and Log output
output_folder: !ref ./results/<seed>
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt
train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>

# FFT parameters
sample_rate: 16000
win_length: 512
hop_length: 128
n_fft: 512
window_fn: !name:torch.hamming_window

# Training Parameters
number_of_epochs: 50
batch_size: 15
learning_rate: 0.001
num_workers: 8
dataloader_options:
    batch_size: !ref <batch_size>
    num_workers: !ref <num_workers>

# The mask operates on log-spectral features, computed using these
# STFT parameters, as well as computing magnitude and log1p.
compute_STFT: !new:speechbrain.processing.features.STFT
    sample_rate: !ref <sample_rate>
    win_length: !ref <win_length>
    hop_length: !ref <hop_length>
    n_fft: !ref <n_fft>
    window_fn: !ref <window_fn>
compute_ISTFT: !new:speechbrain.processing.features.ISTFT
    sample_rate: !ref <sample_rate>
    win_length: !ref <win_length>
    hop_length: !ref <hop_length>
    window_fn: !ref <window_fn>

# To design a custom model, either just edit the simple CustomModel
# class that's listed here, or replace this `!new` call with a line
# pointing to a different file you've defined.
model: !new:gtcrn.GTCRN
    frame_len: !ref <win_length>
    frame_hop: !ref <hop_length>
    window: !ref <window_fn>

# The first object passed to the Brain class is this "Epoch Counter"
# which is saved by the Checkpointer so that training can be resumed
# if it gets interrupted at any point.
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

# Objects in "modules" dict will have their parameters moved to the correct
# device, as well as having train()/eval() called on them by the Brain class.
modules:
    model: !ref <model>

# This optimizer will be constructed by the Brain class after all parameters
# are moved to the correct device. Then it will be added to the checkpointer.
opt_class: !name:torch.optim.Adam
    lr: !ref <learning_rate>
    weight_decay: 0.00001

# This object is used for saving the state of training both so that it
# can be resumed if it gets interrupted, and also so that the best checkpoint
# can be later loaded for evaluation or inference.
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        model: !ref <model>
        counter: !ref <epoch_counter>


sampling_rate: !ref <sample_rate>
audioformat: '*.wav'
audio_length: 10
silence_length: 0.2
total_hours: 1
snr_lower: -5
snr:
snr_upper: 20
randomize_snr: True
target_level_lower: -35
target_level_upper: -15
total_snrlevels: 21
fileindex_start: None
fileindex_end: None
is_test_set: False

noise_dir: !ref <data_untar_folder>/noise
clean_dir: !ref <data_untar_folder>/clean

noisy_destination: !ref <data_folder>/training_set/noisy
clean_destination: !ref <data_folder>/training_set/clean
noise_destination: !ref <data_folder>/training_set/noise
rir_destination: !ref <data_folder>/training_set/rir
log_dir: !ref <data_folder>/logs

add_reverb_data: 0
rir_dir: !ref <data_untar_folder>/dns3/datasets/impulse_responses
# Config: add reverb to clean speech
rir_choice: 3
# 1 for only real rir, 2 for only synthetic rir, 3 (default) use both real and synthetic
lower_t60: 0.3 
# lower bound of t60 range in seconds
upper_t60: 1.3 
# upper bound of t60 range in seconds
percent_for_adding_reverb: 1.0 # percentage of clean speech convolved with RIR

# Unit tests config
snr_test: True
norm_test: True
sampling_rate_test: True
clipping_test: True

unit_tests_log_dir: !ref <data_folder>/unittests_logs
